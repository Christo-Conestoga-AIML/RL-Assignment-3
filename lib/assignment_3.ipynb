{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ad7835",
   "metadata": {},
   "source": [
    "# DQN for Atari Pong (Assignment 3)\n",
    "\n",
    "This notebook implements a modular Deep Q-Network (DQN) for **Pong** using Gymnasium Atari environment. It includes preprocessing (frame crop/downsample/grayscale/normalize), 4-frame stacking as input to a CNN, a replay buffer, target network updates, and facilities to run the required experiments (batch size and target-update frequency).\n",
    "\n",
    "**How to use**: install dependencies, run the training cells (training can take a long time — the notebook provides hooks to run shorter tests), and run the experiment cells to produce the plots required for the assignment.\n",
    "\n",
    "---\n",
    "\n",
    "**Files used**:\n",
    "- `assignment3_utils.py` (should be in the same directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "67964fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:50:45.622187Z",
     "start_time": "2025-11-13T22:50:44.186094Z"
    }
   },
   "source": [
    "import os, random, math, time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Gymnasium import for Atari\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Import preprocessing utilities (assignment3_utils.py must be present)\n",
    "from assignment3_utils import process_frame, transform_reward"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "9197cfe8",
   "metadata": {},
   "source": [
    "## Environment helper\n",
    "We create the Atari Pong environment using Gymnasium. The notebook uses `ALE/PongNoFrameskip-v5` which gives access to raw frames (no frameskip wrapper applied). We will handle frame stacking and preprocessing manually."
   ]
  },
  {
   "cell_type": "code",
   "id": "13d2e113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:50:45.764361Z",
     "start_time": "2025-11-13T22:50:45.640482Z"
    }
   },
   "source": [
    "def make_env(env_id='ALE/Pong-v5'):\n",
    "    # Create environment. Do not render here.\n",
    "    env = gym.make(env_id, frameskip=1, repeat_action_probability=0, full_action_space=False)\n",
    "    return env\n",
    "\n",
    "# quick sanity check (will not run the full env loop)\n",
    "env = make_env()\n",
    "obs, info = env.reset(seed=42)\n",
    "print('Reset obs shape:', obs.shape)\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset obs shape: (210, 160, 3)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9e0563aa",
   "metadata": {},
   "source": [
    "## Preprocessing and frame stacking\n",
    "We rely on `assignment3_utils.process_frame` which returns a processed single frame with shape `(1, H, W, 1)` (batch-like dim, height, width, channel=1). We'll maintain a deque of the last 4 processed frames and concatenate along the channel axis to produce a state of shape `(1, H, W, 4)` which we then transpose to `(1, 4, H, W)` for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "id": "26844718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:50:45.902354Z",
     "start_time": "2025-11-13T22:50:45.779001Z"
    }
   },
   "source": [
    "IMAGE_SHAPE = (84, 80)  # (height, width) after crop+downsample\n",
    "\n",
    "def init_frame_stack():\n",
    "    return deque(maxlen=4)\n",
    "\n",
    "def stack_initial_frames(frame_stack, processed_frame):\n",
    "    # processed_frame is expected to be shape (1, H, W, 1)\n",
    "    frame_stack.append(processed_frame)\n",
    "    # pad with the first frame if needed\n",
    "    while len(frame_stack) < 4:\n",
    "        frame_stack.append(processed_frame)\n",
    "    state = np.concatenate(list(frame_stack), axis=3)  # shape (1, H, W, 4)\n",
    "    return state, frame_stack\n",
    "\n",
    "def append_frame(frame_stack, processed_frame):\n",
    "    frame_stack.append(processed_frame)\n",
    "    state = np.concatenate(list(frame_stack), axis=3)\n",
    "    return state, frame_stack\n",
    "\n",
    "# quick test using the env\n",
    "env = make_env()\n",
    "obs, info = env.reset(seed=0)\n",
    "proc = process_frame(obs, IMAGE_SHAPE)  # shape (1,H,W,1)\n",
    "fs = init_frame_stack()\n",
    "state, fs = stack_initial_frames(fs, proc)\n",
    "print('processed_frame shape:', proc.shape, 'state shape (NHWC):', state.shape)\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_frame shape: (1, 84, 80, 1) state shape (NHWC): (1, 84, 80, 4)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "035b746e",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "id": "984ce449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:50:45.916765Z",
     "start_time": "2025-11-13T22:50:45.912682Z"
    }
   },
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # states expected as numpy arrays with shapes like (1,H,W,4)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # concatenate along batch dim\n",
    "        states = np.concatenate(states, axis=0)          # (B, H, W, C)\n",
    "        next_states = np.concatenate(next_states, axis=0)\n",
    "        return states, np.array(actions), np.array(rewards), next_states, np.array(dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cba81ea7",
   "metadata": {},
   "source": [
    "## DQN network (PyTorch)\n",
    "The CNN follows the common Atari DQN design but adjusted for input shape (4 stacked grayscale frames). Final layer outputs Q-values for the action space (6 actions in Pong)."
   ]
  },
  {
   "cell_type": "code",
   "id": "2b6b8193",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:50:45.936668Z",
     "start_time": "2025-11-13T22:50:45.932939Z"
    }
   },
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels=4, num_actions=6):\n",
    "        super().__init__()\n",
    "        # Input: (batch, 4, 84, 80)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # -> approx (32, 20, 19)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),             # -> approx (64, 9, 8)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),             # -> approx (64, 7, 6)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # compute flattened size by running a dummy tensor\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, input_channels, IMAGE_SHAPE[0], IMAGE_SHAPE[1])\n",
    "            n_flatten = self.features(dummy).shape[1]\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Expect x in float32 and in range approx [-1,1] (assignment util normalizes)\n",
    "        return self.fc(self.features(x))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "2d001602",
   "metadata": {},
   "source": [
    "## Training function (modular)\n",
    "`train_dqn` trains one run with specified hyperparameters and returns recorded metrics and the trained policy network. It does **not** render. It saves optional plots to disk."
   ]
  },
  {
   "cell_type": "code",
   "id": "d866e07a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T22:52:28.182766Z",
     "start_time": "2025-11-13T22:52:28.174759Z"
    }
   },
   "source": [
    "def train_dqn(num_episodes=200,\n",
    "              batch_size=8,\n",
    "              gamma=0.95,\n",
    "              lr=1e-4,\n",
    "              replay_capacity=50000,\n",
    "              target_update_episodes=10,\n",
    "              eps_start=1.0,\n",
    "              eps_decay=0.995,\n",
    "              eps_min=0.05,\n",
    "              device=None,\n",
    "              env_id='ALE/Pong-v5',\n",
    "              save_plots_prefix=\"Plots\",\n",
    "              verbose=True):\n",
    "    \n",
    "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    env = gym.make(env_id, frameskip=1, repeat_action_probability=0, full_action_space=False)\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    policy_net = DQN(input_channels=4, num_actions=num_actions).to(device)\n",
    "    target_net = DQN(input_channels=4, num_actions=num_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = ReplayBuffer(capacity=replay_capacity)\n",
    "    \n",
    "    scores = []\n",
    "    avg5 = []\n",
    "    epsilon = eps_start\n",
    "    total_steps = 0  # cumulative steps across all episodes\n",
    "\n",
    "    for ep in range(1, num_episodes+1):\n",
    "        obs, info = env.reset()\n",
    "        proc = process_frame(obs, IMAGE_SHAPE)  # shape (1,H,W,1)\n",
    "        frame_stack = init_frame_stack()\n",
    "        state, frame_stack = stack_initial_frames(frame_stack, proc)  # state: (1,H,W,4)\n",
    "\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        steps = 0  # steps in current episode\n",
    "\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            # ε-greedy\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                s = torch.tensor(np.transpose(state, (0,3,1,2)), dtype=torch.float32).to(device)  # (1,4,H,W)\n",
    "                with torch.no_grad():\n",
    "                    q = policy_net(s)\n",
    "                action = int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            reward_clipped = transform_reward(reward)\n",
    "            proc_next = process_frame(next_obs, IMAGE_SHAPE)\n",
    "            next_state, frame_stack = append_frame(frame_stack, proc_next)\n",
    "\n",
    "            memory.push(state, action, reward_clipped, next_state, done)\n",
    "            state = next_state\n",
    "            ep_reward += reward_clipped\n",
    "\n",
    "            # learning step\n",
    "            if len(memory) >= batch_size:\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = memory.sample(batch_size)\n",
    "                states_t = torch.tensor(np.transpose(states_b, (0,3,1,2)), dtype=torch.float32).to(device)\n",
    "                next_states_t = torch.tensor(np.transpose(next_states_b, (0,3,1,2)), dtype=torch.float32).to(device)\n",
    "                actions_t = torch.tensor(actions_b, dtype=torch.long).to(device)\n",
    "                rewards_t = torch.tensor(rewards_b, dtype=torch.float32).to(device)\n",
    "                dones_t = torch.tensor(dones_b, dtype=torch.float32).to(device)\n",
    "\n",
    "                q_values = policy_net(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states_t).max(1)[0]\n",
    "                expected = rewards_t + gamma * next_q_values * (1 - dones_t)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, expected)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # update cumulative steps\n",
    "        total_steps += steps\n",
    "\n",
    "        # end episode metrics\n",
    "        scores.append(ep_reward)\n",
    "        avg = np.mean(scores[-5:]) if len(scores) >= 5 else np.mean(scores)\n",
    "        avg5.append(avg)\n",
    "\n",
    "        # decay epsilon\n",
    "        if epsilon > eps_min:\n",
    "            epsilon = max(epsilon * eps_decay, eps_min)\n",
    "\n",
    "        # update target network every N episodes\n",
    "        if ep % target_update_episodes == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Ep {ep}/{num_episodes} | Total Steps: {total_steps} | Reward: {ep_reward:.2f} | Avg5: {avg:.2f} | Eps: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # optional saving plots\n",
    "    if save_plots_prefix:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(scores, label='Score per episode')\n",
    "        plt.plot(avg5, label='Avg last 5')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.title(f'DQN training ({save_plots_prefix})')\n",
    "        plt.savefig(f'{save_plots_prefix}_training.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    return policy_net, scores, avg5\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "48dfe1b2",
   "metadata": {},
   "source": [
    "## Utility: run experiments for the assignment\n",
    "This cell runs (but commented out by default) three experiments you need for the report:\n",
    "\n",
    "1. Default: batch_size=8, target_update=10\n",
    "2. Batch experiment: batch_size=16, target_update=10\n",
    "3. Target update experiment: batch_size=8, target_update=3\n",
    "\n",
    "**Note**: training for many episodes can take hours on CPU. For development you can set `num_episodes=20` or similar. Uncomment and modify before running."
   ]
  },
  {
   "cell_type": "code",
   "id": "75060b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T23:09:40.345883Z",
     "start_time": "2025-11-13T22:52:32.561140Z"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    # -------------------------------\n",
    "    # 1️⃣ Mini-batch size experiment\n",
    "    # -------------------------------\n",
    "    policy_default, scores_default, avg_default = train_dqn(\n",
    "        num_episodes=10, batch_size=8, target_update_episodes=10,\n",
    "        save_plots_prefix='default_run', verbose=True\n",
    "    )\n",
    "    policy_batch16, scores_b16, avg_b16 = train_dqn(\n",
    "        num_episodes=10, batch_size=16, target_update_episodes=10,\n",
    "        save_plots_prefix='batch16_run', verbose=True\n",
    "    )\n",
    "\n",
    "    # Plot metrics for mini-batch size experiment\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(scores_default, label='B=8 (default)')\n",
    "    plt.plot(scores_b16, label='B=16')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('Scores per Episode (Mini-batch Size)')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(avg_default, label='B=8 (default)')\n",
    "    plt.plot(avg_b16, label='B=16')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg last 5')\n",
    "    plt.legend()\n",
    "    plt.title('Average of Last 5 Episodes (Mini-batch Size)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 2️⃣ Target network update frequency experiment\n",
    "    # ------------------------------------------\n",
    "    policy_target3, scores_t3, avg_t3 = train_dqn(\n",
    "        num_episodes=10, batch_size=8, target_update_episodes=3,\n",
    "        save_plots_prefix='target3_run', verbose=True\n",
    "    )\n",
    "    policy_target10, scores_t10, avg_t10 = train_dqn(\n",
    "        num_episodes=10, batch_size=8, target_update_episodes=10,\n",
    "        save_plots_prefix='target10_run', verbose=True\n",
    "    )\n",
    "\n",
    "    # Plot metrics for target update frequency experiment\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(scores_t10, label='T=10 (default)')\n",
    "    plt.plot(scores_t3, label='T=3')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('Scores per Episode (Target Update Frequency)')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(avg_t10, label='T=10 (default)')\n",
    "    plt.plot(avg_t3, label='T=3')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg last 5')\n",
    "    plt.legend()\n",
    "    plt.title('Average of Last 5 Episodes (Target Update Frequency)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1/20 | Reward: -20.00 | Avg5: -20.00 | Eps: 0.995 | Mem: 3481\n",
      "Ep 2/20 | Reward: -21.00 | Avg5: -20.50 | Eps: 0.990 | Mem: 6537\n",
      "Ep 3/20 | Reward: -21.00 | Avg5: -20.67 | Eps: 0.985 | Mem: 9833\n",
      "Ep 4/20 | Reward: -18.00 | Avg5: -20.00 | Eps: 0.980 | Mem: 14122\n",
      "Ep 5/20 | Reward: -21.00 | Avg5: -20.20 | Eps: 0.975 | Mem: 17530\n",
      "Ep 6/20 | Reward: -21.00 | Avg5: -20.40 | Eps: 0.970 | Mem: 21772\n",
      "Ep 7/20 | Reward: -21.00 | Avg5: -20.40 | Eps: 0.966 | Mem: 25255\n",
      "Ep 8/20 | Reward: -18.00 | Avg5: -19.80 | Eps: 0.961 | Mem: 29557\n",
      "Ep 9/20 | Reward: -20.00 | Avg5: -20.20 | Eps: 0.956 | Mem: 33017\n",
      "Ep 10/20 | Reward: -19.00 | Avg5: -19.80 | Eps: 0.951 | Mem: 37164\n",
      "Ep 11/20 | Reward: -21.00 | Avg5: -19.80 | Eps: 0.946 | Mem: 40220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Example short-run usage for debugging (small number of episodes)\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m'\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m      3\u001B[39m     \u001B[38;5;66;03m# Debug short runs (reduce episodes for quick checks)\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     policy_default, scores_default, avg_default = \u001B[43mtrain_dqn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_update_episodes\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_plots_prefix\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdefault_run\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     policy_batch16, scores_b16, avg_b16 = train_dqn(num_episodes=\u001B[32m20\u001B[39m, batch_size=\u001B[32m16\u001B[39m, target_update_episodes=\u001B[32m10\u001B[39m, save_plots_prefix=\u001B[33m'\u001B[39m\u001B[33mbatch16_run\u001B[39m\u001B[33m'\u001B[39m, verbose=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      6\u001B[39m     policy_target3, scores_t3, avg_t3 = train_dqn(num_episodes=\u001B[32m20\u001B[39m, batch_size=\u001B[32m8\u001B[39m, target_update_episodes=\u001B[32m3\u001B[39m, save_plots_prefix=\u001B[33m'\u001B[39m\u001B[33mtarget3_run\u001B[39m\u001B[33m'\u001B[39m, verbose=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 84\u001B[39m, in \u001B[36mtrain_dqn\u001B[39m\u001B[34m(num_episodes, batch_size, gamma, lr, replay_capacity, target_update_episodes, eps_start, eps_decay, eps_min, device, env_id, save_plots_prefix, verbose)\u001B[39m\n\u001B[32m     82\u001B[39m         optimizer.zero_grad()\n\u001B[32m     83\u001B[39m         loss.backward()\n\u001B[32m---> \u001B[39m\u001B[32m84\u001B[39m         \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     86\u001B[39m \u001B[38;5;66;03m# end episode\u001B[39;00m\n\u001B[32m     87\u001B[39m scores.append(ep_reward)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Conestoga\\AI ML\\Level 2\\RL\\RL-Assignment3\\.venv1\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    512\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    513\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    514\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    515\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    520\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Conestoga\\AI ML\\Level 2\\RL\\RL-Assignment3\\.venv1\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     80\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     81\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     84\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Conestoga\\AI ML\\Level 2\\RL\\RL-Assignment3\\.venv1\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    235\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    237\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    238\u001B[39m         group,\n\u001B[32m    239\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    244\u001B[39m         state_steps,\n\u001B[32m    245\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m247\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdecoupled_weight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Conestoga\\AI ML\\Level 2\\RL\\RL-Assignment3\\.venv1\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Conestoga\\AI ML\\Level 2\\RL\\RL-Assignment3\\.venv1\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    951\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m953\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Conestoga\\AI ML\\Level 2\\RL\\RL-Assignment3\\.venv1\\Lib\\site-packages\\torch\\optim\\adam.py:537\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[39m\n\u001B[32m    534\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    535\u001B[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m--> \u001B[39m\u001B[32m537\u001B[39m     \u001B[43mparam\u001B[49m\u001B[43m.\u001B[49m\u001B[43maddcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    539\u001B[39m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n\u001B[32m    540\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m amsgrad \u001B[38;5;129;01mand\u001B[39;00m torch.is_complex(params[i]):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7c123aef",
   "metadata": {},
   "source": [
    "## Notes and next steps\n",
    "- For the full assignment runs, set `num_episodes` to a larger number (e.g. 300-1000) depending on available compute and time.\n",
    "- Save produced PNG plots and include them in the report PDF.\n",
    "- You may want to add model saving (`torch.save`) at checkpoints.\n",
    "- If you need a training resume mechanism, we can add it.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can now:\n",
    "- Run a short debug training here (low episodes) to ensure everything executes in this environment,\n",
    "- Or increase the notebook to include model saving, evaluation (playback), and automatic PDF report skeleton.\n",
    "\n",
    "Which would you like next?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
